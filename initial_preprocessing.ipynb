{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conllu(path):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Sentence boundary\n",
    "            if not line:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                continue\n",
    "\n",
    "            # Skip comments\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            cols = line.split(\"\\t\")\n",
    "\n",
    "            # Skip multi-word tokens like \"1-2 don't\"\n",
    "            if \"-\" in cols[0]:\n",
    "                continue\n",
    "\n",
    "            word = cols[1].lower()\n",
    "            upos = cols[3]\n",
    "\n",
    "            sentence.append((word, upos))\n",
    "\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def train_hmm(train_sentences, threshold=1):\n",
    "    counter = 0\n",
    "    print(\"train_sentences\")\n",
    "    while counter < 2:\n",
    "        print(train_sentences[counter])\n",
    "        counter += 1\n",
    "    print()\n",
    "    # counter = 0\n",
    "    # while counter < 2:\n",
    "    #     print(test_sentences[counter])\n",
    "    #     counter += 1\n",
    "\n",
    "    word_counts = Counter(\n",
    "    word for sent in train_sentences for word, _ in sent\n",
    "    )\n",
    "    print(\"Most Common Words\")\n",
    "    print(word_counts.most_common(10))\n",
    "\n",
    "    \n",
    "    UNK = \"<UNK>\"\n",
    "    threshold = 1 \n",
    "    train_sentences_unk = []\n",
    "\n",
    "    for sent in train_sentences:\n",
    "\n",
    "        new_sent = []\n",
    "\n",
    "        for word, tag in sent:\n",
    "\n",
    "            if word_counts[word] <= threshold:\n",
    "                new_word = UNK\n",
    "            else:\n",
    "                new_word = word\n",
    "\n",
    "            new_sent.append((new_word, tag))\n",
    "\n",
    "        train_sentences_unk.append(new_sent)\n",
    "\n",
    "        \n",
    "    vocab = set()\n",
    "\n",
    "    for sent in train_sentences_unk:\n",
    "        for word, tag in sent:\n",
    "            vocab.add(word)\n",
    "\n",
    "    print(\"Vocabulary size:\", len(vocab))\n",
    "    print(\"<UNK> in vocabulary:\", UNK in vocab)\n",
    "\n",
    "\n",
    "    #Emission probabilities\n",
    "    emission_counts = Counter()\n",
    "    tag_counts_emission = Counter()\n",
    "\n",
    "    for sent in train_sentences_unk:\n",
    "        for word, tag in sent:\n",
    "            emission_counts[(tag, word)] += 1\n",
    "            tag_counts_emission[tag] += 1\n",
    "    # print(len(tag_counts_emission))\n",
    "    emission_probs = {}\n",
    "\n",
    "    for (tag, word), count in emission_counts.items():\n",
    "        emission_probs[(tag, word)] = count / tag_counts_emission[tag]\n",
    "    # print(emission_probs)\n",
    "    tags = list(tag_counts_emission.keys())\n",
    "\n",
    "    \n",
    "    #Initial Probabilities (P(tag/START))\n",
    "\n",
    "\n",
    "    initial_tag_counts = Counter()\n",
    "    total_sentences = len(train_sentences_unk)\n",
    "\n",
    "    for sent in train_sentences_unk:\n",
    "        #check the tag of first word of each sentence\n",
    "        first_tag = sent[0][1]\n",
    "        initial_tag_counts[first_tag] += 1\n",
    "    # print(initial_tag_counts)\n",
    "    # print()\n",
    "    #calculate prob that a tag occurs at the start\n",
    "    # initial_probs = {\n",
    "    #     tag: count / total_sentences\n",
    "    #     for tag, count in initial_tag_counts.items()\n",
    "    # }\n",
    "    initial_probs = {\n",
    "    tag: initial_tag_counts.get(tag, 0) / total_sentences\n",
    "    for tag in tags\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    sorted_data = dict(sorted(initial_probs.items(), key=lambda item: item[1]))\n",
    "    print(f\"initial_probs(sorted_data): {sorted_data}\")\n",
    "    # print(initial_probs)  \n",
    "\n",
    "\n",
    "\n",
    "    #Transition probabilities\n",
    "    transition_counts = Counter()\n",
    "    tag_counts = Counter()   # denominator\n",
    "\n",
    "    for sent in train_sentences_unk:\n",
    "        for i in range(len(sent) - 1):\n",
    "            tag_i = sent[i][1]\n",
    "            tag_j = sent[i + 1][1]\n",
    "\n",
    "            transition_counts[(tag_i, tag_j)] += 1\n",
    "            tag_counts[tag_i] += 1\n",
    "    transition_probs = {}\n",
    "\n",
    "    for (tag_i, tag_j), count in transition_counts.items():\n",
    "        transition_probs[(tag_i, tag_j)] = count / tag_counts[tag_i]\n",
    "\n",
    "    # print(transition_probs)\n",
    "\n",
    "\n",
    "    return initial_probs, transition_probs, emission_probs, vocab, tags\n",
    "\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_checks(initial_probs,transition_probs,emission_probs):\n",
    "#basic checks\n",
    "    print(sum(initial_probs.values()))\n",
    "    print(sum(\n",
    "        prob for (t1, _), prob in transition_probs.items()\n",
    "        if t1 == \"NOUN\"\n",
    "    )\n",
    "    )\n",
    "    print(sum(\n",
    "        prob for (t, _), prob in emission_probs.items()\n",
    "        if t == \"NOUN\"\n",
    "    )\n",
    "    )\n",
    "\n",
    "    # total\n",
    "    # for (tag, word), prob in emission_probs.items():\n",
    "    #     if tag == \"PUNCT\":\n",
    "    #         # print(f\"word={word} prob={prob}\")\n",
    "    #         total=total+prob\n",
    "    # print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9412254,
     "sourceId": 14729507,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
