{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conllu(path):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Sentence boundary\n",
    "            if not line:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                continue\n",
    "\n",
    "            # Skip comments\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            cols = line.split(\"\\t\")\n",
    "\n",
    "            # Skip multi-word tokens like \"1-2 don't\"\n",
    "            if \"-\" in cols[0]:\n",
    "                continue\n",
    "\n",
    "            word = cols[1].lower()\n",
    "            upos = cols[3]\n",
    "\n",
    "            sentence.append((word, upos))\n",
    "\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update to local paths to test locally\n",
    "# TRAIN_PATH = \"/kaggle/input/nlp-datasets/en_ewt-ud-train.conllu\"\n",
    "# TEST_PATH  = \"/kaggle/input/nlp-datasets/en_ewt-ud-test.conllu\"\n",
    "TRAIN_PATH = \"en_ewt-ud-train.conllu\"\n",
    "TEST_PATH  = \"en_ewt-ud-test.conllu\"\n",
    "train_sentences = read_conllu(TRAIN_PATH)\n",
    "test_sentences  = read_conllu(TEST_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('al', 'PROPN'), ('-', 'PUNCT'), ('zaman', 'PROPN'), (':', 'PUNCT'), ('american', 'ADJ'), ('forces', 'NOUN'), ('killed', 'VERB'), ('shaikh', 'PROPN'), ('abdullah', 'PROPN'), ('al', 'PROPN'), ('-', 'PUNCT'), ('ani', 'PROPN'), (',', 'PUNCT'), ('the', 'DET'), ('preacher', 'NOUN'), ('at', 'ADP'), ('the', 'DET'), ('mosque', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('town', 'NOUN'), ('of', 'ADP'), ('qaim', 'PROPN'), (',', 'PUNCT'), ('near', 'ADP'), ('the', 'DET'), ('syrian', 'ADJ'), ('border', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('[', 'PUNCT'), ('this', 'DET'), ('killing', 'NOUN'), ('of', 'ADP'), ('a', 'DET'), ('respected', 'ADJ'), ('cleric', 'NOUN'), ('will', 'AUX'), ('be', 'AUX'), ('causing', 'VERB'), ('us', 'PRON'), ('trouble', 'NOUN'), ('for', 'ADP'), ('years', 'NOUN'), ('to', 'PART'), ('come', 'VERB'), ('.', 'PUNCT'), (']', 'PUNCT')]\n",
      "\n",
      "[('what', 'PRON'), ('if', 'SCONJ'), ('google', 'PROPN'), ('morphed', 'VERB'), ('into', 'ADP'), ('googleos', 'PROPN'), ('?', 'PUNCT')]\n",
      "[('what', 'PRON'), ('if', 'SCONJ'), ('google', 'PROPN'), ('expanded', 'VERB'), ('on', 'ADP'), ('its', 'PRON'), ('search', 'NOUN'), ('-', 'PUNCT'), ('engine', 'NOUN'), ('(', 'PUNCT'), ('and', 'CCONJ'), ('now', 'ADV'), ('e-mail', 'NOUN'), (')', 'PUNCT'), ('wares', 'NOUN'), ('into', 'ADP'), ('a', 'DET'), ('full', 'ADV'), ('-', 'PUNCT'), ('fledged', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('?', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "while counter < 2:\n",
    "    print(train_sentences[counter])\n",
    "    counter += 1\n",
    "print()\n",
    "counter = 0\n",
    "while counter < 2:\n",
    "    print(test_sentences[counter])\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 9075), ('.', 8640), (',', 7021), ('to', 5137), ('and', 5002), ('a', 3782), ('of', 3622), ('i', 3380), ('in', 3112), ('is', 2241)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter(\n",
    "    word for sent in train_sentences for word, _ in sent\n",
    ")\n",
    "\n",
    "print(word_counts.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': 7.971938775510203e-05, 'PART': 0.00470344387755102, 'SYM': 0.0074936224489795915, 'CCONJ': 0.02311862244897959, 'AUX': 0.028858418367346938, 'INTJ': 0.03228635204081633, 'PUNCT': 0.035235969387755105, 'SCONJ': 0.035634566326530615, 'NUM': 0.039142219387755105, 'ADJ': 0.04097576530612245, 'ADP': 0.04320790816326531, 'VERB': 0.06050701530612245, 'NOUN': 0.06194196428571429, 'ADV': 0.07653061224489796, 'DET': 0.10044642857142858, 'PROPN': 0.12771045918367346, 'PRON': 0.28212691326530615}\n"
     ]
    }
   ],
   "source": [
    "#Initial Probabilities (P(tag/START))\n",
    "from collections import Counter\n",
    "\n",
    "initial_tag_counts = Counter()\n",
    "total_sentences = len(train_sentences)\n",
    "\n",
    "for sent in train_sentences:\n",
    "    #check the tag of first word of each sentence\n",
    "    first_tag = sent[0][1]\n",
    "    initial_tag_counts[first_tag] += 1\n",
    "# print(initial_tag_counts)\n",
    "# print()\n",
    "#calculate prob that a tag occurs at the start\n",
    "initial_probs = {\n",
    "    tag: count / total_sentences\n",
    "    for tag, count in initial_tag_counts.items()\n",
    "}\n",
    "\n",
    "sorted_data = dict(sorted(initial_probs.items(), key=lambda item: item[1]))\n",
    "print(sorted_data)\n",
    "# print(initial_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transition probabilities\n",
    "transition_counts = Counter()\n",
    "tag_counts = Counter()   # denominator\n",
    "\n",
    "for sent in train_sentences:\n",
    "    for i in range(len(sent) - 1):\n",
    "        tag_i = sent[i][1]\n",
    "        tag_j = sent[i + 1][1]\n",
    "\n",
    "        transition_counts[(tag_i, tag_j)] += 1\n",
    "        tag_counts[tag_i] += 1\n",
    "transition_probs = {}\n",
    "\n",
    "for (tag_i, tag_j), count in transition_counts.items():\n",
    "    transition_probs[(tag_i, tag_j)] = count / tag_counts[tag_i]\n",
    "\n",
    "# print(transition_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emission probabilities\n",
    "emission_counts = Counter()\n",
    "tag_counts_emission = Counter()\n",
    "\n",
    "for sent in train_sentences:\n",
    "    for word, tag in sent:\n",
    "        emission_counts[(tag, word)] += 1\n",
    "        tag_counts_emission[tag] += 1\n",
    "# print(len(tag_counts_emission))\n",
    "emission_probs = {}\n",
    "\n",
    "for (tag, word), count in emission_counts.items():\n",
    "    emission_probs[(tag, word)] = count / tag_counts_emission[tag]\n",
    "# print(emission_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#basic checks\n",
    "print(sum(initial_probs.values()))\n",
    "print(sum(\n",
    "    prob for (t1, _), prob in transition_probs.items()\n",
    "    if t1 == \"NOUN\"\n",
    ")\n",
    ")\n",
    "print(sum(\n",
    "    prob for (t, _), prob in emission_probs.items()\n",
    "    if t == \"NOUN\"\n",
    ")\n",
    ")\n",
    "\n",
    "# total\n",
    "# for (tag, word), prob in emission_probs.items():\n",
    "#     if tag == \"PUNCT\":\n",
    "#         # print(f\"word={word} prob={prob}\")\n",
    "#         total=total+prob\n",
    "# print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brute force way \n",
    "# if sentences are long there will be many possible tag sequences and overall many iterations to check so we use viterbi algorithm to optimize\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "def sequence_probability(words, tag_seq,initial_probs,transition_probs,emission_probs):\n",
    "#for computing probabilities of a tag sequence in brute force way\n",
    "    prob = 1.0\n",
    "\n",
    "    word = words[0]\n",
    "    prob *= initial_probs.get(tag_seq[0], 0)\n",
    "    prob *= emission_probs.get((tag_seq[0], word), 0)\n",
    "\n",
    "    # example for a three letter word\n",
    "    \n",
    "    # P(t1)×P(w1∣t1)×P(t2∣t1)×P(w2∣t2)×P(t3∣t2)×P(w3∣t3)\n",
    "    # p(t1)-initial_prob\n",
    "\n",
    "    for i in range(1, len(words)):\n",
    "        prev_tag = tag_seq[i-1]\n",
    "        curr_tag = tag_seq[i]\n",
    "\n",
    "        word = words[i]\n",
    "\n",
    "        prob *= transition_probs.get((prev_tag, curr_tag), 0)\n",
    "        prob *= emission_probs.get((curr_tag, word), 0)\n",
    "\n",
    "    return prob\n",
    "\n",
    "\n",
    "def brute_force(words,all_tags,initial_probs,transition_probs,emission_probs):\n",
    "\n",
    "    best_prob = -1\n",
    "    best_seq = None\n",
    "\n",
    "    # generate probability for every possible tag sequence\n",
    "    for tag_seq in itertools.product(all_tags, repeat=len(words)):\n",
    "\n",
    "        prob = sequence_probability(words,tag_seq,initial_probs,transition_probs,emission_probs)\n",
    "\n",
    "        if prob > best_prob:\n",
    "            best_prob = prob\n",
    "            best_seq = tag_seq\n",
    "\n",
    "    return list(best_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(words,all_tags,initial_probs,transition_probs,emission_probs):\n",
    "    T = len(words)\n",
    "    N = len(all_tags)\n",
    "    \n",
    "    V = {}\n",
    "    B = {}\n",
    "\n",
    "    for tag in all_tags:\n",
    "        V[(tag, 0)] = (\n",
    "            initial_probs.get(tag, 0) *\n",
    "            emission_probs.get((tag, words[0]), 0)\n",
    "        )\n",
    "        B[(tag, 0)] = None\n",
    "        \n",
    "    # Vt(tagj ) = max(tagi(Vt−1(tagi) × P(tagj | tagi) × P(wordt| tagj ))\n",
    "    \n",
    "    for i in range(1, len(words)):\n",
    "        for curr_tag in all_tags:\n",
    "\n",
    "            best_prob = 0\n",
    "            best_prev = None\n",
    "\n",
    "            for prev_tag in all_tags:\n",
    "                prob = (V[(prev_tag, i-1)] *transition_probs.get((prev_tag, curr_tag), 0) *emission_probs.get((curr_tag, words[i]), 0))\n",
    "                \n",
    "                if prob > best_prob:\n",
    "                    best_prob = prob\n",
    "                    best_prev = prev_tag\n",
    "            V[(curr_tag, i)] = best_prob\n",
    "            B[(curr_tag, i)] = best_prev\n",
    "                #backtracking matric stores the best prev tag (tag that leads to the maximum Viterbi probability)s at the particular position for a particular tag\n",
    "    \n",
    "    #to choose which final tag gives the highest probability overall\n",
    "    \n",
    "    last_pos = len(words) - 1    \n",
    "    # Return the item for which V[(t, last_pos) is largest\n",
    "\n",
    "    best_last_tag = None\n",
    "    best_last_prob = -1\n",
    "\n",
    "    for tag in all_tags:\n",
    "        if V[(tag, last_pos)] > best_last_prob:\n",
    "            best_last_prob = V[(tag, last_pos)]\n",
    "            best_last_tag = tag\n",
    "\n",
    "    # the viterbi maxtrix contain the max probability that can be obtained with a specific tag sequence that ends with that tag at that position.\n",
    "    # we backtrack from the final  tag with highest probability\n",
    "    best_tags = [best_last_tag]\n",
    "\n",
    "    for i in range(last_pos, 0, -1):\n",
    "        prev = B.get((best_tags[-1], i))\n",
    "\n",
    "        if prev is None:\n",
    "            # fallback: choose tag with max probability at previous position\n",
    "            prev = max(\n",
    "                all_tags,\n",
    "                key=lambda t: V[(t, i-1)]\n",
    "            )\n",
    "\n",
    "        best_tags.append(prev)\n",
    "\n",
    "    best_tags.reverse()\n",
    "\n",
    "\n",
    "    \n",
    "    return best_tags\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(test_sentences,initial_probs,transition_probs,emission_probs,brute_force,viterbi,max_examples=6):\n",
    "\n",
    "    all_tags = list(tag_counts_emission.keys())\n",
    "    shown = 0\n",
    "\n",
    "    for sent in test_sentences:\n",
    "\n",
    "        words = [w for w, _ in sent]\n",
    "        gold  = [t for _, t in sent]\n",
    "\n",
    "        print(\"Sentence:\", words)\n",
    "        print(\"Gold tags:\", gold)\n",
    "        \n",
    "        if len(words) < 7:\n",
    "            bf_pred = brute_force(words,all_tags,initial_probs,transition_probs,emission_probs)\n",
    "\n",
    "            bf_correct = sum(g == p for g, p in zip(gold, bf_pred))\n",
    "            bf_acc = bf_correct / len(gold)\n",
    "\n",
    "            print(\"Brute Force:\", bf_pred)\n",
    "            print(\"BF Accuracy:\", round(bf_acc, 3))\n",
    "        else:\n",
    "            print(\"Brute Force: skipped (sentence too long)\")\n",
    "\n",
    "\n",
    "        vit_pred = viterbi(words,all_tags,initial_probs,transition_probs,emission_probs)\n",
    "\n",
    "        vit_correct = sum(g == p for g, p in zip(gold, vit_pred))\n",
    "        vit_acc = vit_correct / len(gold)\n",
    "\n",
    "        print(\"Viterbi:    \", vit_pred)\n",
    "        print(\"VT Accuracy:\", round(vit_acc, 3))\n",
    "        print()\n",
    "        shown += 1\n",
    "        if shown >= max_examples:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['what', 'if', 'google', 'morphed', 'into', 'googleos', '?']\n",
      "Gold tags: ['PRON', 'SCONJ', 'PROPN', 'VERB', 'ADP', 'PROPN', 'PUNCT']\n",
      "Brute Force: skipped (sentence too long)\n",
      "Viterbi:     ['PRON', 'SCONJ', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN']\n",
      "VT Accuracy: 0.571\n",
      "\n",
      "Sentence: ['what', 'if', 'google', 'expanded', 'on', 'its', 'search', '-', 'engine', '(', 'and', 'now', 'e-mail', ')', 'wares', 'into', 'a', 'full', '-', 'fledged', 'operating', 'system', '?']\n",
      "Gold tags: ['PRON', 'SCONJ', 'PROPN', 'VERB', 'ADP', 'PRON', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'CCONJ', 'ADV', 'NOUN', 'PUNCT', 'NOUN', 'ADP', 'DET', 'ADV', 'PUNCT', 'ADJ', 'NOUN', 'NOUN', 'PUNCT']\n",
      "Brute Force: skipped (sentence too long)\n",
      "Viterbi:     ['PRON', 'SCONJ', 'PROPN', 'VERB', 'ADP', 'PRON', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'CCONJ', 'ADV', 'NOUN', 'PUNCT', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN']\n",
      "VT Accuracy: 0.609\n",
      "\n",
      "Sentence: ['[', 'via', 'microsoft', 'watch', 'from', 'mary', 'jo', 'foley', ']']\n",
      "Gold tags: ['PUNCT', 'ADP', 'PROPN', 'PROPN', 'ADP', 'PROPN', 'PROPN', 'PROPN', 'PUNCT']\n",
      "Brute Force: skipped (sentence too long)\n",
      "Viterbi:     ['PUNCT', 'ADP', 'PROPN', 'VERB', 'ADP', 'PROPN', 'PROPN', 'PROPN', 'PROPN']\n",
      "VT Accuracy: 0.778\n",
      "\n",
      "Sentence: ['(', 'and', ',', 'by', 'the', 'way', ',', 'is', 'anybody', 'else', 'just', 'a', 'little', 'nostalgic', 'for', 'the', 'days', 'when', 'that', 'was', 'a', 'good', 'thing', '?', ')']\n",
      "Gold tags: ['PUNCT', 'CCONJ', 'PUNCT', 'ADP', 'DET', 'NOUN', 'PUNCT', 'AUX', 'PRON', 'ADV', 'ADV', 'DET', 'ADJ', 'ADJ', 'ADP', 'DET', 'NOUN', 'ADV', 'PRON', 'AUX', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'PUNCT']\n",
      "Brute Force: skipped (sentence too long)\n",
      "Viterbi:     ['PUNCT', 'CCONJ', 'PUNCT', 'ADP', 'DET', 'NOUN', 'PUNCT', 'AUX', 'PRON', 'ADV', 'ADV', 'DET', 'ADJ', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN']\n",
      "VT Accuracy: 0.52\n",
      "\n",
      "Sentence: ['this', 'buzzmachine', 'post', 'argues', 'that', 'google', \"'s\", 'rush', 'toward', 'ubiquity', 'might', 'backfire', '--', 'which', 'we', \"'ve\", 'all', 'heard', 'before', ',', 'but', 'it', \"'s\", 'particularly', 'well', '-', 'put', 'in', 'this', 'post', '.']\n",
      "Gold tags: ['DET', 'PROPN', 'NOUN', 'VERB', 'SCONJ', 'PROPN', 'PART', 'NOUN', 'ADP', 'NOUN', 'AUX', 'VERB', 'PUNCT', 'PRON', 'PRON', 'AUX', 'ADV', 'VERB', 'ADV', 'PUNCT', 'CCONJ', 'PRON', 'AUX', 'ADV', 'ADV', 'PUNCT', 'VERB', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
      "Brute Force: skipped (sentence too long)\n",
      "Viterbi:     ['PRON', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN']\n",
      "VT Accuracy: 0.065\n",
      "\n",
      "Sentence: ['google', 'is', 'a', 'nice', 'search', 'engine', '.']\n",
      "Gold tags: ['PROPN', 'AUX', 'DET', 'ADJ', 'NOUN', 'NOUN', 'PUNCT']\n",
      "Brute Force: skipped (sentence too long)\n",
      "Viterbi:     ['PROPN', 'AUX', 'DET', 'ADJ', 'NOUN', 'NOUN', 'PUNCT']\n",
      "VT Accuracy: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "debug(test_sentences,initial_probs,transition_probs,emission_probs,brute_force,viterbi,max_examples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9412254,
     "sourceId": 14729507,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
