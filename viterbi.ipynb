{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f11bb908-1579-446d-bf6e-3e67a67afd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4d390a3-e444-456e-b12e-5a394e0c5c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"<UNK>\"\n",
    "\n",
    "def handle_unknown(word,vocab):\n",
    "    if word not in vocab:\n",
    "        return UNK\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09ce0844-7f21-4fc3-804e-35693b73084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#brute force way \n",
    "# if sentences are long there will be many possible tag sequences and overall many iterations to check so we use viterbi algorithm to optimize\n",
    "\n",
    "\n",
    "def sequence_probability(words, tag_seq,initial_probs,transition_probs,emission_probs):\n",
    "#for computing probabilities of a tag sequence in brute force way\n",
    "    prob = 1.0\n",
    "\n",
    "    word = words[0]\n",
    "    prob *= initial_probs.get(tag_seq[0], 0)\n",
    "    prob *= emission_probs.get((tag_seq[0], word), 0)\n",
    "\n",
    "    # example for a three letter word\n",
    "    \n",
    "    # P(t1)×P(w1∣t1)×P(t2∣t1)×P(w2∣t2)×P(t3∣t2)×P(w3∣t3)\n",
    "    # p(t1)-initial_prob\n",
    "\n",
    "    for i in range(1, len(words)):\n",
    "        prev_tag = tag_seq[i-1]\n",
    "        curr_tag = tag_seq[i]\n",
    "\n",
    "        word = words[i]\n",
    "\n",
    "        prob *= transition_probs.get((prev_tag, curr_tag), 0)\n",
    "        prob *= emission_probs.get((curr_tag, word), 0)\n",
    "\n",
    "    return prob\n",
    "\n",
    "\n",
    "def brute_force(words,all_tags,initial_probs,transition_probs,emission_probs):\n",
    "    words = [handle_unknown(word) for word in words]\n",
    "    best_prob = -1\n",
    "    best_seq = None\n",
    "\n",
    "    # generate probability for every possible tag sequence\n",
    "    for tag_seq in itertools.product(all_tags, repeat=len(words)):\n",
    "\n",
    "        prob = sequence_probability(words,tag_seq,initial_probs,transition_probs,emission_probs)\n",
    "\n",
    "        if prob > best_prob:\n",
    "            best_prob = prob\n",
    "            best_seq = tag_seq\n",
    "\n",
    "    return list(best_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7294f7ad-a05b-475d-a75d-d1dea008dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(words,all_tags,initial_probs,transition_probs,emission_probs,vocab):\n",
    "    words = [handle_unknown(word,vocab) for word in words]\n",
    "    T = len(words)\n",
    "    N = len(all_tags)\n",
    "    \n",
    "    V = {}\n",
    "    B = {}\n",
    "\n",
    "    for tag in all_tags:\n",
    "        V[(tag, 0)] = (\n",
    "            initial_probs.get(tag, 0) *\n",
    "            emission_probs.get((tag, words[0]), 0)\n",
    "        )\n",
    "        B[(tag, 0)] = None\n",
    "        \n",
    "    # Vt(tagj ) = max(tagi(Vt−1(tagi) × P(tagj | tagi) × P(wordt| tagj ))\n",
    "    \n",
    "    for i in range(1, len(words)):\n",
    "        for curr_tag in all_tags:\n",
    "\n",
    "            best_prob = 0\n",
    "            best_prev = None\n",
    "\n",
    "            for prev_tag in all_tags:\n",
    "                prob = (V[(prev_tag, i-1)] *transition_probs.get((prev_tag, curr_tag), 0) *emission_probs.get((curr_tag, words[i]), 0))\n",
    "                \n",
    "                if prob > best_prob:\n",
    "                    best_prob = prob\n",
    "                    best_prev = prev_tag\n",
    "            V[(curr_tag, i)] = best_prob\n",
    "            B[(curr_tag, i)] = best_prev\n",
    "                #backtracking matric stores the best prev tag (tag that leads to the maximum Viterbi probability)s at the particular position for a particular tag\n",
    "    \n",
    "    #to choose which final tag gives the highest probability overall\n",
    "    \n",
    "    last_pos = len(words) - 1    \n",
    "    # Return the item for which V[(t, last_pos) is largest\n",
    "\n",
    "    best_last_tag = None\n",
    "    best_last_prob = -1\n",
    "\n",
    "    for tag in all_tags:\n",
    "        if V[(tag, last_pos)] > best_last_prob:\n",
    "            best_last_prob = V[(tag, last_pos)]\n",
    "            best_last_tag = tag\n",
    "\n",
    "    # the viterbi maxtrix contain the max probability that can be obtained with a specific tag sequence that ends with that tag at that position.\n",
    "    # we backtrack from the final  tag with highest probability\n",
    "    best_tags = [best_last_tag]\n",
    "\n",
    "    for i in range(last_pos, 0, -1):\n",
    "        prev = B.get((best_tags[-1], i))\n",
    "\n",
    "        if prev is None:\n",
    "            # fallback: choose tag with max probability at previous position\n",
    "            prev = max(\n",
    "                all_tags,\n",
    "                key=lambda t: V[(t, i-1)]\n",
    "            )\n",
    "\n",
    "        best_tags.append(prev)\n",
    "\n",
    "    best_tags.reverse()\n",
    "\n",
    "\n",
    "    \n",
    "    return best_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4940e4c5-ab80-4acb-9006-4d0325b3faf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
