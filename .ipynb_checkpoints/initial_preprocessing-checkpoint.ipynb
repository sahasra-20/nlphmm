{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conllu(path):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Sentence boundary\n",
    "            if not line:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                continue\n",
    "\n",
    "            # Skip comments\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            cols = line.split(\"\\t\")\n",
    "\n",
    "            # Skip multi-word tokens like \"1-2 don't\"\n",
    "            if \"-\" in cols[0]:\n",
    "                continue\n",
    "\n",
    "            word = cols[1].lower()\n",
    "            upos = cols[3]\n",
    "\n",
    "            sentence.append((word, upos))\n",
    "\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update to local paths to test locally\n",
    "# TRAIN_PATH = \"/kaggle/input/nlp-datasets/en_ewt-ud-train.conllu\"\n",
    "# TEST_PATH  = \"/kaggle/input/nlp-datasets/en_ewt-ud-test.conllu\"\n",
    "TRAIN_PATH = \"en_ewt-ud-train.conllu\"\n",
    "TEST_PATH  = \"en_ewt-ud-test.conllu\"\n",
    "train_sentences = read_conllu(TRAIN_PATH)\n",
    "test_sentences  = read_conllu(TEST_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('al', 'PROPN'), ('-', 'PUNCT'), ('zaman', 'PROPN'), (':', 'PUNCT'), ('american', 'ADJ'), ('forces', 'NOUN'), ('killed', 'VERB'), ('shaikh', 'PROPN'), ('abdullah', 'PROPN'), ('al', 'PROPN'), ('-', 'PUNCT'), ('ani', 'PROPN'), (',', 'PUNCT'), ('the', 'DET'), ('preacher', 'NOUN'), ('at', 'ADP'), ('the', 'DET'), ('mosque', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('town', 'NOUN'), ('of', 'ADP'), ('qaim', 'PROPN'), (',', 'PUNCT'), ('near', 'ADP'), ('the', 'DET'), ('syrian', 'ADJ'), ('border', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('[', 'PUNCT'), ('this', 'DET'), ('killing', 'NOUN'), ('of', 'ADP'), ('a', 'DET'), ('respected', 'ADJ'), ('cleric', 'NOUN'), ('will', 'AUX'), ('be', 'AUX'), ('causing', 'VERB'), ('us', 'PRON'), ('trouble', 'NOUN'), ('for', 'ADP'), ('years', 'NOUN'), ('to', 'PART'), ('come', 'VERB'), ('.', 'PUNCT'), (']', 'PUNCT')]\n",
      "\n",
      "[('what', 'PRON'), ('if', 'SCONJ'), ('google', 'PROPN'), ('morphed', 'VERB'), ('into', 'ADP'), ('googleos', 'PROPN'), ('?', 'PUNCT')]\n",
      "[('what', 'PRON'), ('if', 'SCONJ'), ('google', 'PROPN'), ('expanded', 'VERB'), ('on', 'ADP'), ('its', 'PRON'), ('search', 'NOUN'), ('-', 'PUNCT'), ('engine', 'NOUN'), ('(', 'PUNCT'), ('and', 'CCONJ'), ('now', 'ADV'), ('e-mail', 'NOUN'), (')', 'PUNCT'), ('wares', 'NOUN'), ('into', 'ADP'), ('a', 'DET'), ('full', 'ADV'), ('-', 'PUNCT'), ('fledged', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('?', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "while counter < 2:\n",
    "    print(train_sentences[counter])\n",
    "    counter += 1\n",
    "print()\n",
    "counter = 0\n",
    "while counter < 2:\n",
    "    print(test_sentences[counter])\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 9075), ('.', 8640), (',', 7021), ('to', 5137), ('and', 5002), ('a', 3782), ('of', 3622), ('i', 3380), ('in', 3112), ('is', 2241)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter(\n",
    "    word for sent in train_sentences for word, _ in sent\n",
    ")\n",
    "\n",
    "print(word_counts.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': 7.971938775510203e-05, 'PART': 0.00470344387755102, 'SYM': 0.0074936224489795915, 'CCONJ': 0.02311862244897959, 'AUX': 0.028858418367346938, 'INTJ': 0.03228635204081633, 'PUNCT': 0.035235969387755105, 'SCONJ': 0.035634566326530615, 'NUM': 0.039142219387755105, 'ADJ': 0.04097576530612245, 'ADP': 0.04320790816326531, 'VERB': 0.06050701530612245, 'NOUN': 0.06194196428571429, 'ADV': 0.07653061224489796, 'DET': 0.10044642857142858, 'PROPN': 0.12771045918367346, 'PRON': 0.28212691326530615}\n"
     ]
    }
   ],
   "source": [
    "#Initial Probabilities (P(tag/START))\n",
    "from collections import Counter\n",
    "\n",
    "initial_tag_counts = Counter()\n",
    "total_sentences = len(train_sentences)\n",
    "\n",
    "for sent in train_sentences:\n",
    "    #check the tag of first word of each sentence\n",
    "    first_tag = sent[0][1]\n",
    "    initial_tag_counts[first_tag] += 1\n",
    "# print(initial_tag_counts)\n",
    "# print()\n",
    "#calculate prob that a tag occurs at the start\n",
    "initial_probs = {\n",
    "    tag: count / total_sentences\n",
    "    for tag, count in initial_tag_counts.items()\n",
    "}\n",
    "\n",
    "sorted_data = dict(sorted(initial_probs.items(), key=lambda item: item[1]))\n",
    "print(sorted_data)\n",
    "# print(initial_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transition probabilities\n",
    "transition_counts = Counter()\n",
    "tag_counts = Counter()   # denominator\n",
    "\n",
    "for sent in train_sentences:\n",
    "    for i in range(len(sent) - 1):\n",
    "        tag_i = sent[i][1]\n",
    "        tag_j = sent[i + 1][1]\n",
    "\n",
    "        transition_counts[(tag_i, tag_j)] += 1\n",
    "        tag_counts[tag_i] += 1\n",
    "transition_probs = {}\n",
    "\n",
    "for (tag_i, tag_j), count in transition_counts.items():\n",
    "    transition_probs[(tag_i, tag_j)] = count / tag_counts[tag_i]\n",
    "\n",
    "# print(transition_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emission probabilities\n",
    "emission_counts = Counter()\n",
    "tag_counts_emission = Counter()\n",
    "\n",
    "for sent in train_sentences:\n",
    "    for word, tag in sent:\n",
    "        emission_counts[(tag, word)] += 1\n",
    "        tag_counts_emission[tag] += 1\n",
    "# print(len(tag_counts_emission))\n",
    "emission_probs = {}\n",
    "\n",
    "for (tag, word), count in emission_counts.items():\n",
    "    emission_probs[(tag, word)] = count / tag_counts_emission[tag]\n",
    "# print(emission_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#basic checks\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minitial_probs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(\n\u001b[0;32m      4\u001b[0m     prob \u001b[38;5;28;01mfor\u001b[39;00m (t1, _), prob \u001b[38;5;129;01min\u001b[39;00m transition_probs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t1 \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOUN\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(\n\u001b[0;32m      9\u001b[0m     prob \u001b[38;5;28;01mfor\u001b[39;00m (t, _), prob \u001b[38;5;129;01min\u001b[39;00m emission_probs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOUN\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "#basic checks\n",
    "print(sum(initial_probs.values()))\n",
    "print(sum(\n",
    "    prob for (t1, _), prob in transition_probs.items()\n",
    "    if t1 == \"NOUN\"\n",
    ")\n",
    ")\n",
    "print(sum(\n",
    "    prob for (t, _), prob in emission_probs.items()\n",
    "    if t == \"NOUN\"\n",
    ")\n",
    ")\n",
    "\n",
    "# total\n",
    "# for (tag, word), prob in emission_probs.items():\n",
    "#     if tag == \"PUNCT\":\n",
    "#         # print(f\"word={word} prob={prob}\")\n",
    "#         total=total+prob\n",
    "# print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['see', 'http://www.gulf-news.com/articles/news.asp?articleid=97508']\n",
      "crt_tag:      ['VERB', 'PROPN']\n",
      "Pred:      ['PROPN', 'PROPN']\n",
      "Accuracy:  0.5\n",
      "\n",
      "Sentence: ['i.e', '.']\n",
      "crt_tag:      ['VERB', 'PROPN']\n",
      "Pred:      ['ADV', 'PUNCT']\n",
      "Accuracy:  1.0\n",
      "\n",
      "Sentence: ['...']\n",
      "crt_tag:      ['VERB', 'PROPN']\n",
      "Pred:      ['PUNCT']\n",
      "Accuracy:  0.0\n",
      "\n",
      "Sentence: ['wtf', 'is', 'this', '?']\n",
      "crt_tag:      ['VERB', 'PROPN']\n",
      "Pred:      ['PROPN', 'PROPN', 'PROPN', 'PROPN']\n",
      "Accuracy:  0.0\n",
      "\n",
      "Sentence: ['i', \"'m\", 'the', 'king']\n",
      "crt_tag:      ['VERB', 'PROPN']\n",
      "Pred:      ['PRON', 'AUX', 'DET', 'PROPN']\n",
      "Accuracy:  0.75\n",
      "\n",
      "Sentence: ['yeah']\n",
      "crt_tag:      ['VERB', 'PROPN']\n",
      "Pred:      ['INTJ']\n",
      "Accuracy:  1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#brute force way \n",
    "# if sentences are long there will be many possible tag sequences and overall many iterations to check so we use viterbi algorithm to optimize\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "def sequence_probability(words, tag_seq,initial_probs,transition_probs,emission_probs):\n",
    "#for computing probabilities of a tag sequence in brute force way\n",
    "    prob = 1.0\n",
    "\n",
    "    word = words[0]\n",
    "    prob *= initial_probs[tag_seq[0]]\n",
    "    prob *= emission_probs.get((tag_seq[0], word), 0)\n",
    "\n",
    "        # example for a three letter word\n",
    "    \n",
    "    # P(t1)×P(w1∣t1)×P(t2∣t1)×P(w2∣t2)×P(t3∣t2)×P(w3∣t3)\n",
    "    # p(t1)-initial_prob\n",
    "    \n",
    "\n",
    "    for i in range(1, len(words)):\n",
    "        prev_tag = tag_seq[i-1]\n",
    "        curr_tag = tag_seq[i]\n",
    "\n",
    "        word = words[i]\n",
    "\n",
    "        transition_probs.get((prev_tag, curr_tag), 0)\n",
    "        prob *= emission_probs.get((curr_tag, word), 0)\n",
    "\n",
    "    return prob\n",
    "\n",
    "\n",
    "def brute_force_decode(words,all_tags,initial_probs,transition_probs,emission_probs):\n",
    "\n",
    "    best_prob = -1\n",
    "    best_seq = None\n",
    "\n",
    "    # generate probability for every possible tag sequence\n",
    "    for tag_seq in itertools.product(all_tags, repeat=len(words)):\n",
    "\n",
    "        prob = sequence_probability(\n",
    "            words,\n",
    "            tag_seq,\n",
    "            initial_probs,\n",
    "            transition_probs,\n",
    "            emission_probs\n",
    "        )\n",
    "\n",
    "        if prob > best_prob:\n",
    "            best_prob = prob\n",
    "            best_seq = tag_seq\n",
    "\n",
    "    return list(best_seq)\n",
    "\n",
    "    \n",
    "all_tags = list(initial_probs.keys())\n",
    "# print(all_tags) all possible tags\n",
    "correct = 0\n",
    "total = 0\n",
    "c=0;\n",
    "for sent in test_sentences:\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    if len(sent) < 5: \n",
    "\n",
    "        words = [w for w,_ in sent]\n",
    "        crt_tag= [t for _,t in sent]\n",
    "\n",
    "        pred = brute_force_decode(\n",
    "            words,\n",
    "            all_tags,\n",
    "            initial_probs,\n",
    "            transition_probs,\n",
    "            emission_probs\n",
    "        )\n",
    "\n",
    "\n",
    "        for t, p in zip(crt_tag, pred):\n",
    "            if t == p:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "        acc = correct / len(crt_tag)\n",
    "\n",
    "        print(\"Sentence:\", words)\n",
    "        print(\"crt_tag:     \", gold)\n",
    "        print(\"Pred:     \", pred)\n",
    "        print(\"Accuracy: \", round(acc, 3))\n",
    "        print()\n",
    "        c+=1\n",
    "        \n",
    "        if c>5:\n",
    "            break\n",
    "        \n",
    "        \n",
    "\n",
    "    # print(\"Accuracy (short sentences only):\", correct/total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9412254,
     "sourceId": 14729507,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
